main_func                  : here
filter                     : DPiso
order                      : GQL
engine                     : LFTJ
model                      : our
do_train                   : True
do_validation              : False
dataset                    : MSRC_21
noiseratio                 : 0
do_test                    : False
num_cs_refinements         : 3
search_constraints         : isomorphic
thresh_throwaway_qsa       : 1e-08
reward_method              : exp_depth
reward_method_exp_coeff    : 100
reward_method_normalize    : True
debug_embeddings_every_k_iters : None
loss_type                  : mse_bounded
regularization             : 0.05
add_gnd_truth              : add_gnd_truth_at_start
mark_best_as_true          : True
eps_decay_config           : {'start_eps': 0.0, 'end_eps': 0.0, 'num_iters': 5}
train_dataset              : imsm-dblp_unlabeled
val_dataset                : imsm-dblp_unlabeled
train_sample_size_li       : [None]
val_sample_size_li         : [None]
train_subgroup             : list
train_subgroup_list        : ['dense_64']
train_path_indices         : [[0, 5], [0, 5], [0, 5], [0, 5], [0, 50], [0, 50], [0, 50]]
val_subgroup               : dense_64
val_subgroup_list          : None
val_path_indices           : [95, 100]
pretrain                   : False
imitation                  : False
show_precision             : False
buffer_size                : 128
batch_size                 : 1
print_buffer_stats         : True
prune_trivial_rewards      : False
print_rpreds_stats         : True
glsearch                   : False
num_epochs_per_learn       : 128
num_outer_epoch            : 64
skip_data_leakage          : False
load_model                 : None
append_ldf                 : True
val_every_games            : 1
save_every                 : 1000
matching_order             : nn
use_is_early_pruning       : False
MCTS_train                 : True
MCTS_test                  : False
MCTS_printQU               : True
MCTS_num_iters_max         : 120
MCTS_num_iters_per_action  : 10.0
MCTS_temp                  : 5.0
MCTS_temp_inner            : 1.0
MCTS_cpuct                 : 10.0
MCTS_eps_in_U              : 1e-08
MCTS_backup_to_real_root   : True
d_enc                      : 32
encoder_type               : mlp
use_NN_for_u_score         : False
dvn_config                 : {'preencoder': {'type': 'concat+mlp'}, 'encoder': {'type': 'GNNConsensusEncoder', 'gnn_type': 'OurGMNv2', 'gnn_subtype': 'graphgps', 'hidden_gnn_dims': [32, 32, 32, 32], 'shared_gnn_weights': False, 'shared_encoder': False, 'q2t': True, 't2q': True, 'consensus_cfg_li': None}, 'decoder_dvn': {'type': 'Query', 'simple_decoder': {'mlp_att': [32, 8, 1], 'mlp_val': [32, 32], 'mlp_final': [32, 32, 16, 8, 4, 1]}}, 'decoder_policy': {'type': 'bilinear_custom', 'similarity_decoder': {'mlp_in_dims': [32, 32], 'mlp_out_dims': [64, 32, 16, 8, 1], 'g_emb': 32}}}
cache_embeddings           : True
k_sample_cross_graph       : None
regret_iters_train         : 1
regret_iters_test          : 1
use_node_mask_diameter     : False
num_iters_threshold        : -1
timeout                    : 300
time                       : 2023-10-20_11-44-50
time_analysis              : False
learning_timeout           : 120
timeout_val                : 120
num_iters_threshold_val    : 200
plot_tree                  : False
plot_solution              : False
plot_logs                  : False
save_search                : False
device                     : cuda:5
ckptID                     : 20000
encoder_structure          : encoder1
graphgps_config_path       : MSRC_21_GateGCN_LapPE_RWSE.yaml
lr                         : 1e-06
modelID                    : 2024-04-12_00-00-44
order_Gq                   : RI
dataset_type               : dense
sys_path                   : ['/home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm/', '/home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm/uclasm/', '/home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm/GraphGPS/']
fix_randomness             : True
random_seed                : 123
skip_if_action_space_less_than : None
apply_norm                 : True
ckpt                       : 
train_mode                 : RL
user                       : kli16
hostname                   : vector.cs.gsu.edu
ckpt_folder                : ckpt_RL
dim                        : 27
subgraph_node_num          : 16_32
ts                         : 2024-08-15T17-12-17.773079

python /home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm_LapPE/NSUBS/model/OurSGM/main.py --MCTS_backup_to_real_root=True  --MCTS_cpuct=10.0  --MCTS_eps_in_U=1e-08  --MCTS_num_iters_max=120  --MCTS_num_iters_per_action=10.0  --MCTS_printQU=True  --MCTS_temp=5.0  --MCTS_temp_inner=1.0  --MCTS_test=False  --MCTS_train=True  --add_gnd_truth=add_gnd_truth_at_start  --append_ldf=True  --apply_norm=True  --batch_size=1  --buffer_size=128  --cache_embeddings=True  --ckpt=  --ckptID=20000  --ckpt_folder=ckpt_RL  --d_enc=32  --dataset=MSRC_21  --dataset_type=dense  --debug_embeddings_every_k_iters=None  --device=cuda:5  --dim=27  --do_test=False  --do_train=True  --do_validation=False  --dvn_config={'preencoder': {'type': 'concat+mlp'}, 'encoder': {'type': 'GNNConsensusEncoder', 'gnn_type': 'OurGMNv2', 'gnn_subtype': 'graphgps', 'hidden_gnn_dims': [32, 32, 32, 32], 'shared_gnn_weights': False, 'shared_encoder': False, 'q2t': True, 't2q': True, 'consensus_cfg_li': None}, 'decoder_dvn': {'type': 'Query', 'simple_decoder': {'mlp_att': [32, 8, 1], 'mlp_val': [32, 32], 'mlp_final': [32, 32, 16, 8, 4, 1]}}, 'decoder_policy': {'type': 'bilinear_custom', 'similarity_decoder': {'mlp_in_dims': [32, 32], 'mlp_out_dims': [64, 32, 16, 8, 1], 'g_emb': 32}}}  --encoder_structure=encoder1  --encoder_type=mlp  --engine=LFTJ  --eps_decay_config={'start_eps': 0.0, 'end_eps': 0.0, 'num_iters': 5}  --filter=DPiso  --fix_randomness=True  --glsearch=False  --graphgps_config_path=MSRC_21_GateGCN_LapPE_RWSE.yaml  --hostname=vector.cs.gsu.edu  --imitation=False  --k_sample_cross_graph=None  --learning_timeout=120  --load_model=None  --loss_type=mse_bounded  --lr=1e-06  --main_func=here  --mark_best_as_true=True  --matching_order=nn  --model=our  --modelID=2024-04-12_00-00-44  --noiseratio=0  --num_cs_refinements=3  --num_epochs_per_learn=128  --num_iters_threshold=-1  --num_iters_threshold_val=200  --num_outer_epoch=64  --order=GQL  --order_Gq=RI  --plot_logs=False  --plot_solution=False  --plot_tree=False  --pretrain=False  --print_buffer_stats=True  --print_rpreds_stats=True  --prune_trivial_rewards=False  --random_seed=123  --regret_iters_test=1  --regret_iters_train=1  --regularization=0.05  --reward_method=exp_depth  --reward_method_exp_coeff=100  --reward_method_normalize=True  --save_every=1000  --save_search=False  --search_constraints=isomorphic  --show_precision=False  --skip_data_leakage=False  --skip_if_action_space_less_than=None  --subgraph_node_num=16_32  --sys_path=['/home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm/', '/home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm/uclasm/', '/home/kli16/ISM_custom/esm_NSUBS_RWSE_LapPE/esm/GraphGPS/']  --thresh_throwaway_qsa=1e-08  --time=2023-10-20_11-44-50  --time_analysis=False  --timeout=300  --timeout_val=120  --train_dataset=imsm-dblp_unlabeled  --train_mode=RL  --train_path_indices=[[0, 5], [0, 5], [0, 5], [0, 5], [0, 50], [0, 50], [0, 50]]  --train_sample_size_li=[None]  --train_subgroup=list  --train_subgroup_list=['dense_64']  --use_NN_for_u_score=False  --use_is_early_pruning=False  --use_node_mask_diameter=False  --user=kli16  --val_dataset=imsm-dblp_unlabeled  --val_every_games=1  --val_path_indices=[95, 100]  --val_sample_size_li=[None]  --val_subgroup=dense_64  --val_subgroup_list=None

model:
DVN_wrapper(
  (dvn): DVN(
    (pre_encoder): PreEncoderConcatSelectedOneHotAndMLP(
      (lap_encoder_q): LapPENodeEncoder(
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (lap_encoder_t): LapPENodeEncoder(
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (mlp_q): Linear(in_features=29, out_features=16, bias=True)
      (mlp_t): Linear(in_features=29, out_features=16, bias=True)
    )
    (encoder): GNNConsensusEncoder(
      (gnn_wrapper_li): ModuleList(
        (0-3): 4 x GNNWrapper(
          (gnnm): OurGMNCustomWrapper(
            (gmn_inter): OurGMNCustomInter(
              (transformer_conv): TransformerConv(32, 32, heads=4)
            )
            (gmn_intra): OurGMNCustomIntra(
              (gnn_t): GPSLayer(
                summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
                )
                (norm1_local): LayerNorm(32, affine=True, mode=graph)
                (norm1_attn): LayerNorm(32, affine=True, mode=graph)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
                (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
                (act_fn_ff): ReLU()
                (norm2): LayerNorm(32, affine=True, mode=graph)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
              (gnn_q): GPSLayer(
                summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
                )
                (norm1_local): LayerNorm(32, affine=True, mode=graph)
                (norm1_attn): LayerNorm(32, affine=True, mode=graph)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
                (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
                (act_fn_ff): ReLU()
                (norm2): LayerNorm(32, affine=True, mode=graph)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (jk): JumpingKnowledge(max)
    )
    (gt_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 32)
    )
    (gq_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 32)
    )
    (decoder_policy): BilinearDecoder(
      (encoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (decoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=96, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=1, bias=True)
        )
      )
    )
    (decoder_value): QueryDecoder(
      (mlp_att): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=1, bias=True)
        )
      )
      (mlp_val): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (mlp_final): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=4, bias=True)
          (4): Linear(in_features=4, out_features=1, bias=True)
        )
      )
      (norm): NormalizeAttention()
    )
    (norm_li): ModuleList(
      (0-3): 4 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (mlp_final): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): Linear(in_features=16, out_features=8, bias=True)
      (3): Linear(in_features=8, out_features=4, bias=True)
      (4): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)


Details:
DVN_wrapper(
  (dvn): DVN(
    (pre_encoder): PreEncoderConcatSelectedOneHotAndMLP(
      (lap_encoder_q): LapPENodeEncoder(
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (lap_encoder_t): LapPENodeEncoder(
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (mlp_q): Linear(in_features=29, out_features=16, bias=True)
      (mlp_t): Linear(in_features=29, out_features=16, bias=True)
    )
    (encoder): GNNConsensusEncoder(
      (gnn_wrapper_li): ModuleList(
        (0-3): 4 x GNNWrapper(
          (gnnm): OurGMNCustomWrapper(
            (gmn_inter): OurGMNCustomInter(
              (transformer_conv): TransformerConv(32, 32, heads=4)
            )
            (gmn_intra): OurGMNCustomIntra(
              (gnn_t): GPSLayer(
                summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
                )
                (norm1_local): LayerNorm(32, affine=True, mode=graph)
                (norm1_attn): LayerNorm(32, affine=True, mode=graph)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
                (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
                (act_fn_ff): ReLU()
                (norm2): LayerNorm(32, affine=True, mode=graph)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
              (gnn_q): GPSLayer(
                summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
                )
                (norm1_local): LayerNorm(32, affine=True, mode=graph)
                (norm1_attn): LayerNorm(32, affine=True, mode=graph)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
                (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
                (act_fn_ff): ReLU()
                (norm2): LayerNorm(32, affine=True, mode=graph)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (jk): JumpingKnowledge(max)
    )
    (gt_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 32)
    )
    (gq_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 32)
    )
    (decoder_policy): BilinearDecoder(
      (encoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (decoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=96, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=1, bias=True)
        )
      )
    )
    (decoder_value): QueryDecoder(
      (mlp_att): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=1, bias=True)
        )
      )
      (mlp_val): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (mlp_final): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=32, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=4, bias=True)
          (4): Linear(in_features=4, out_features=1, bias=True)
        )
      )
      (norm): NormalizeAttention()
    )
    (norm_li): ModuleList(
      (0-3): 4 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (mlp_final): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): Linear(in_features=16, out_features=8, bias=True)
      (3): Linear(in_features=8, out_features=4, bias=True)
      (4): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)

dvn
DVN(
  (pre_encoder): PreEncoderConcatSelectedOneHotAndMLP(
    (lap_encoder_q): LapPENodeEncoder(
      (linear_A): Linear(in_features=2, out_features=32, bias=True)
      (pe_encoder): Sequential(
        (0): ReLU()
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): ReLU()
      )
    )
    (lap_encoder_t): LapPENodeEncoder(
      (linear_A): Linear(in_features=2, out_features=32, bias=True)
      (pe_encoder): Sequential(
        (0): ReLU()
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): ReLU()
      )
    )
    (mlp_q): Linear(in_features=29, out_features=16, bias=True)
    (mlp_t): Linear(in_features=29, out_features=16, bias=True)
  )
  (encoder): GNNConsensusEncoder(
    (gnn_wrapper_li): ModuleList(
      (0-3): 4 x GNNWrapper(
        (gnnm): OurGMNCustomWrapper(
          (gmn_inter): OurGMNCustomInter(
            (transformer_conv): TransformerConv(32, 32, heads=4)
          )
          (gmn_intra): OurGMNCustomIntra(
            (gnn_t): GPSLayer(
              summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
              (local_model): GatedGCNLayer()
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
              )
              (norm1_local): LayerNorm(32, affine=True, mode=graph)
              (norm1_attn): LayerNorm(32, affine=True, mode=graph)
              (dropout_local): Dropout(p=0.0, inplace=False)
              (dropout_attn): Dropout(p=0.0, inplace=False)
              (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
              (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
              (act_fn_ff): ReLU()
              (norm2): LayerNorm(32, affine=True, mode=graph)
              (ff_dropout1): Dropout(p=0.0, inplace=False)
              (ff_dropout2): Dropout(p=0.0, inplace=False)
            )
            (gnn_q): GPSLayer(
              summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
              (local_model): GatedGCNLayer()
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
              )
              (norm1_local): LayerNorm(32, affine=True, mode=graph)
              (norm1_attn): LayerNorm(32, affine=True, mode=graph)
              (dropout_local): Dropout(p=0.0, inplace=False)
              (dropout_attn): Dropout(p=0.0, inplace=False)
              (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
              (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
              (act_fn_ff): ReLU()
              (norm2): LayerNorm(32, affine=True, mode=graph)
              (ff_dropout1): Dropout(p=0.0, inplace=False)
              (ff_dropout2): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (jk): JumpingKnowledge(max)
  )
  (gt_egde_encoder): DummyEdgeEncoder(
    (encoder): Embedding(1, 32)
  )
  (gq_egde_encoder): DummyEdgeEncoder(
    (encoder): Embedding(1, 32)
  )
  (decoder_policy): BilinearDecoder(
    (encoder): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (decoder): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=96, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
    )
  )
  (decoder_value): QueryDecoder(
    (mlp_att): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=32, out_features=8, bias=True)
        (1): Linear(in_features=8, out_features=1, bias=True)
      )
    )
    (mlp_val): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (mlp_final): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=4, bias=True)
        (4): Linear(in_features=4, out_features=1, bias=True)
      )
    )
    (norm): NormalizeAttention()
  )
  (norm_li): ModuleList(
    (0-3): 4 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
)

dvn.pre_encoder
PreEncoderConcatSelectedOneHotAndMLP(
  (lap_encoder_q): LapPENodeEncoder(
    (linear_A): Linear(in_features=2, out_features=32, bias=True)
    (pe_encoder): Sequential(
      (0): ReLU()
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): ReLU()
    )
  )
  (lap_encoder_t): LapPENodeEncoder(
    (linear_A): Linear(in_features=2, out_features=32, bias=True)
    (pe_encoder): Sequential(
      (0): ReLU()
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): ReLU()
    )
  )
  (mlp_q): Linear(in_features=29, out_features=16, bias=True)
  (mlp_t): Linear(in_features=29, out_features=16, bias=True)
)

dvn.pre_encoder.lap_encoder_q
LapPENodeEncoder(
  (linear_A): Linear(in_features=2, out_features=32, bias=True)
  (pe_encoder): Sequential(
    (0): ReLU()
    (1): Linear(in_features=32, out_features=16, bias=True)
    (2): ReLU()
  )
)

dvn.pre_encoder.lap_encoder_q.linear_A
Linear(in_features=2, out_features=32, bias=True)

dvn.pre_encoder.lap_encoder_q.pe_encoder
Sequential(
  (0): ReLU()
  (1): Linear(in_features=32, out_features=16, bias=True)
  (2): ReLU()
)

dvn.pre_encoder.lap_encoder_q.pe_encoder.0
ReLU()

dvn.pre_encoder.lap_encoder_q.pe_encoder.1
Linear(in_features=32, out_features=16, bias=True)

dvn.pre_encoder.lap_encoder_q.pe_encoder.2
ReLU()

dvn.pre_encoder.lap_encoder_t
LapPENodeEncoder(
  (linear_A): Linear(in_features=2, out_features=32, bias=True)
  (pe_encoder): Sequential(
    (0): ReLU()
    (1): Linear(in_features=32, out_features=16, bias=True)
    (2): ReLU()
  )
)

dvn.pre_encoder.lap_encoder_t.linear_A
Linear(in_features=2, out_features=32, bias=True)

dvn.pre_encoder.lap_encoder_t.pe_encoder
Sequential(
  (0): ReLU()
  (1): Linear(in_features=32, out_features=16, bias=True)
  (2): ReLU()
)

dvn.pre_encoder.lap_encoder_t.pe_encoder.0
ReLU()

dvn.pre_encoder.lap_encoder_t.pe_encoder.1
Linear(in_features=32, out_features=16, bias=True)

dvn.pre_encoder.lap_encoder_t.pe_encoder.2
ReLU()

dvn.pre_encoder.mlp_q
Linear(in_features=29, out_features=16, bias=True)

dvn.pre_encoder.mlp_t
Linear(in_features=29, out_features=16, bias=True)

dvn.encoder
GNNConsensusEncoder(
  (gnn_wrapper_li): ModuleList(
    (0-3): 4 x GNNWrapper(
      (gnnm): OurGMNCustomWrapper(
        (gmn_inter): OurGMNCustomInter(
          (transformer_conv): TransformerConv(32, 32, heads=4)
        )
        (gmn_intra): OurGMNCustomIntra(
          (gnn_t): GPSLayer(
            summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
            (local_model): GatedGCNLayer()
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
            )
            (norm1_local): LayerNorm(32, affine=True, mode=graph)
            (norm1_attn): LayerNorm(32, affine=True, mode=graph)
            (dropout_local): Dropout(p=0.0, inplace=False)
            (dropout_attn): Dropout(p=0.0, inplace=False)
            (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
            (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
            (act_fn_ff): ReLU()
            (norm2): LayerNorm(32, affine=True, mode=graph)
            (ff_dropout1): Dropout(p=0.0, inplace=False)
            (ff_dropout2): Dropout(p=0.0, inplace=False)
          )
          (gnn_q): GPSLayer(
            summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
            (local_model): GatedGCNLayer()
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
            )
            (norm1_local): LayerNorm(32, affine=True, mode=graph)
            (norm1_attn): LayerNorm(32, affine=True, mode=graph)
            (dropout_local): Dropout(p=0.0, inplace=False)
            (dropout_attn): Dropout(p=0.0, inplace=False)
            (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
            (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
            (act_fn_ff): ReLU()
            (norm2): LayerNorm(32, affine=True, mode=graph)
            (ff_dropout1): Dropout(p=0.0, inplace=False)
            (ff_dropout2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (jk): JumpingKnowledge(max)
)

dvn.encoder.gnn_wrapper_li
ModuleList(
  (0-3): 4 x GNNWrapper(
    (gnnm): OurGMNCustomWrapper(
      (gmn_inter): OurGMNCustomInter(
        (transformer_conv): TransformerConv(32, 32, heads=4)
      )
      (gmn_intra): OurGMNCustomIntra(
        (gnn_t): GPSLayer(
          summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
          (local_model): GatedGCNLayer()
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
          )
          (norm1_local): LayerNorm(32, affine=True, mode=graph)
          (norm1_attn): LayerNorm(32, affine=True, mode=graph)
          (dropout_local): Dropout(p=0.0, inplace=False)
          (dropout_attn): Dropout(p=0.0, inplace=False)
          (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
          (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
          (act_fn_ff): ReLU()
          (norm2): LayerNorm(32, affine=True, mode=graph)
          (ff_dropout1): Dropout(p=0.0, inplace=False)
          (ff_dropout2): Dropout(p=0.0, inplace=False)
        )
        (gnn_q): GPSLayer(
          summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
          (local_model): GatedGCNLayer()
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
          )
          (norm1_local): LayerNorm(32, affine=True, mode=graph)
          (norm1_attn): LayerNorm(32, affine=True, mode=graph)
          (dropout_local): Dropout(p=0.0, inplace=False)
          (dropout_attn): Dropout(p=0.0, inplace=False)
          (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
          (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
          (act_fn_ff): ReLU()
          (norm2): LayerNorm(32, affine=True, mode=graph)
          (ff_dropout1): Dropout(p=0.0, inplace=False)
          (ff_dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.0
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (transformer_conv): TransformerConv(32, 32, heads=4)
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (transformer_conv): TransformerConv(32, 32, heads=4)
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter
OurGMNCustomInter(
  (transformer_conv): TransformerConv(32, 32, heads=4)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.transformer_conv
TransformerConv(32, 32, heads=4)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.transformer_conv.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.transformer_conv.lin_key
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.transformer_conv.lin_query
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.transformer_conv.lin_value
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.transformer_conv.lin_skip
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (transformer_conv): TransformerConv(32, 32, heads=4)
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (transformer_conv): TransformerConv(32, 32, heads=4)
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter
OurGMNCustomInter(
  (transformer_conv): TransformerConv(32, 32, heads=4)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.transformer_conv
TransformerConv(32, 32, heads=4)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.transformer_conv.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.transformer_conv.lin_key
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.transformer_conv.lin_query
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.transformer_conv.lin_value
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.transformer_conv.lin_skip
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (transformer_conv): TransformerConv(32, 32, heads=4)
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (transformer_conv): TransformerConv(32, 32, heads=4)
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter
OurGMNCustomInter(
  (transformer_conv): TransformerConv(32, 32, heads=4)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.transformer_conv
TransformerConv(32, 32, heads=4)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.transformer_conv.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.transformer_conv.lin_key
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.transformer_conv.lin_query
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.transformer_conv.lin_value
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.transformer_conv.lin_skip
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (transformer_conv): TransformerConv(32, 32, heads=4)
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (norm1_local): LayerNorm(32, affine=True, mode=graph)
        (norm1_attn): LayerNorm(32, affine=True, mode=graph)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
        (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
        (act_fn_ff): ReLU()
        (norm2): LayerNorm(32, affine=True, mode=graph)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (transformer_conv): TransformerConv(32, 32, heads=4)
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1_local): LayerNorm(32, affine=True, mode=graph)
      (norm1_attn): LayerNorm(32, affine=True, mode=graph)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
      (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
      (act_fn_ff): ReLU()
      (norm2): LayerNorm(32, affine=True, mode=graph)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter
OurGMNCustomInter(
  (transformer_conv): TransformerConv(32, 32, heads=4)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.transformer_conv
TransformerConv(32, 32, heads=4)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.transformer_conv.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.transformer_conv.lin_key
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.transformer_conv.lin_query
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.transformer_conv.lin_value
Linear(32, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.transformer_conv.lin_skip
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
    )
    (norm1_local): LayerNorm(32, affine=True, mode=graph)
    (norm1_attn): LayerNorm(32, affine=True, mode=graph)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
    (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
    (act_fn_ff): ReLU()
    (norm2): LayerNorm(32, affine=True, mode=graph)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=32, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
  )
  (norm1_local): LayerNorm(32, affine=True, mode=graph)
  (norm1_attn): LayerNorm(32, affine=True, mode=graph)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=32, out_features=64, bias=True)
  (ff_linear2): Linear(in_features=64, out_features=32, bias=True)
  (act_fn_ff): ReLU()
  (norm2): LayerNorm(32, affine=True, mode=graph)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.A
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.B
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.C
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.D
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.E
Linear(32, 32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.norm1_local
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.norm1_attn
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=32, out_features=64, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=64, out_features=32, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.norm2
LayerNorm(32, affine=True, mode=graph)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.jk
JumpingKnowledge(max)

dvn.gt_egde_encoder
DummyEdgeEncoder(
  (encoder): Embedding(1, 32)
)

dvn.gt_egde_encoder.encoder
Embedding(1, 32)

dvn.gq_egde_encoder
DummyEdgeEncoder(
  (encoder): Embedding(1, 32)
)

dvn.gq_egde_encoder.encoder
Embedding(1, 32)

dvn.decoder_policy
BilinearDecoder(
  (encoder): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=32, out_features=32, bias=True)
    )
  )
  (decoder): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=96, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): Linear(in_features=16, out_features=8, bias=True)
      (3): Linear(in_features=8, out_features=1, bias=True)
    )
  )
)

dvn.decoder_policy.encoder
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=32, out_features=32, bias=True)
  )
)

dvn.decoder_policy.encoder.activation
ELU(alpha=1.0)

dvn.decoder_policy.encoder.layers
ModuleList(
  (0): Linear(in_features=32, out_features=32, bias=True)
)

dvn.decoder_policy.encoder.layers.0
Linear(in_features=32, out_features=32, bias=True)

dvn.decoder_policy.decoder
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=96, out_features=32, bias=True)
    (1): Linear(in_features=32, out_features=16, bias=True)
    (2): Linear(in_features=16, out_features=8, bias=True)
    (3): Linear(in_features=8, out_features=1, bias=True)
  )
)

dvn.decoder_policy.decoder.activation
ELU(alpha=1.0)

dvn.decoder_policy.decoder.layers
ModuleList(
  (0): Linear(in_features=96, out_features=32, bias=True)
  (1): Linear(in_features=32, out_features=16, bias=True)
  (2): Linear(in_features=16, out_features=8, bias=True)
  (3): Linear(in_features=8, out_features=1, bias=True)
)

dvn.decoder_policy.decoder.layers.0
Linear(in_features=96, out_features=32, bias=True)

dvn.decoder_policy.decoder.layers.1
Linear(in_features=32, out_features=16, bias=True)

dvn.decoder_policy.decoder.layers.2
Linear(in_features=16, out_features=8, bias=True)

dvn.decoder_policy.decoder.layers.3
Linear(in_features=8, out_features=1, bias=True)

dvn.decoder_value
QueryDecoder(
  (mlp_att): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=32, out_features=8, bias=True)
      (1): Linear(in_features=8, out_features=1, bias=True)
    )
  )
  (mlp_val): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=32, out_features=32, bias=True)
    )
  )
  (mlp_final): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): Linear(in_features=16, out_features=8, bias=True)
      (3): Linear(in_features=8, out_features=4, bias=True)
      (4): Linear(in_features=4, out_features=1, bias=True)
    )
  )
  (norm): NormalizeAttention()
)

dvn.decoder_value.mlp_att
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=32, out_features=8, bias=True)
    (1): Linear(in_features=8, out_features=1, bias=True)
  )
)

dvn.decoder_value.mlp_att.activation
ELU(alpha=1.0)

dvn.decoder_value.mlp_att.layers
ModuleList(
  (0): Linear(in_features=32, out_features=8, bias=True)
  (1): Linear(in_features=8, out_features=1, bias=True)
)

dvn.decoder_value.mlp_att.layers.0
Linear(in_features=32, out_features=8, bias=True)

dvn.decoder_value.mlp_att.layers.1
Linear(in_features=8, out_features=1, bias=True)

dvn.decoder_value.mlp_val
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=32, out_features=32, bias=True)
  )
)

dvn.decoder_value.mlp_val.activation
ELU(alpha=1.0)

dvn.decoder_value.mlp_val.layers
ModuleList(
  (0): Linear(in_features=32, out_features=32, bias=True)
)

dvn.decoder_value.mlp_val.layers.0
Linear(in_features=32, out_features=32, bias=True)

dvn.decoder_value.mlp_final
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): Linear(in_features=32, out_features=16, bias=True)
    (2): Linear(in_features=16, out_features=8, bias=True)
    (3): Linear(in_features=8, out_features=4, bias=True)
    (4): Linear(in_features=4, out_features=1, bias=True)
  )
)

dvn.decoder_value.mlp_final.activation
ELU(alpha=1.0)

dvn.decoder_value.mlp_final.layers
ModuleList(
  (0): Linear(in_features=32, out_features=32, bias=True)
  (1): Linear(in_features=32, out_features=16, bias=True)
  (2): Linear(in_features=16, out_features=8, bias=True)
  (3): Linear(in_features=8, out_features=4, bias=True)
  (4): Linear(in_features=4, out_features=1, bias=True)
)

dvn.decoder_value.mlp_final.layers.0
Linear(in_features=32, out_features=32, bias=True)

dvn.decoder_value.mlp_final.layers.1
Linear(in_features=32, out_features=16, bias=True)

dvn.decoder_value.mlp_final.layers.2
